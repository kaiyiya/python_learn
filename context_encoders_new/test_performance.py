#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import torch
import time
from model.net import ContextEncoderGenerator, ContextEncoderDiscriminator, weights_init

def benchmark_forward_pass():
    """æµ‹è¯•å‰å‘ä¼ æ’­æ€§èƒ½"""
    print("æµ‹è¯•å‰å‘ä¼ æ’­æ€§èƒ½...")
    
    class TestArgs:
        def __init__(self):
            self.ngpu = 1
            self.nc = 3
            self.nef = 64
            self.ngf = 64
            self.ndf = 64
            self.n_bottleneck = 4000
            self.image_size = 128
            self.overlap_pred = 4
    
    opt = TestArgs()
    
    # åˆ›å»ºæ¨¡å‹
    generator = ContextEncoderGenerator(opt)
    discriminator = ContextEncoderDiscriminator(opt)
    
    # åº”ç”¨æƒé‡åˆå§‹åŒ–
    generator.apply(weights_init)
    discriminator.apply(weights_init)
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
    batch_sizes = [1, 2, 4, 8]
    
    for batch_size in batch_sizes:
        print(f"\næµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        corrupted_images = torch.randn(batch_size, 3, 128, 128)
        real_centers = torch.randn(batch_size, 3, 64, 64)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(5):
                fake_centers = generator(corrupted_images)
                d_real = discriminator(real_centers)
                d_fake = discriminator(fake_centers)
        
        # æµ‹è¯•ç”Ÿæˆå™¨æ€§èƒ½
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(100):
                fake_centers = generator(corrupted_images)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        gen_time = time.time() - start_time
        
        # æµ‹è¯•åˆ¤åˆ«å™¨æ€§èƒ½
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(100):
                d_real = discriminator(real_centers)
                d_fake = discriminator(fake_centers)
        
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        disc_time = time.time() - start_time
        
        print(f"  ç”Ÿæˆå™¨: {gen_time/100*1000:.2f} ms/batch")
        print(f"  åˆ¤åˆ«å™¨: {disc_time/100*1000:.2f} ms/batch")
        print(f"  æ€»æ—¶é—´: {(gen_time+disc_time)/100*1000:.2f} ms/batch")

def benchmark_training_step():
    """æµ‹è¯•è®­ç»ƒæ­¥éª¤æ€§èƒ½"""
    print("\næµ‹è¯•è®­ç»ƒæ­¥éª¤æ€§èƒ½...")
    
    class TestArgs:
        def __init__(self):
            self.ngpu = 1
            self.nc = 3
            self.nef = 64
            self.ngf = 64
            self.ndf = 64
            self.n_bottleneck = 4000
            self.image_size = 128
            self.overlap_pred = 4
            self.wtl2 = 0.998
            self.overlapL2Weight = 10
    
    opt = TestArgs()
    
    # åˆ›å»ºæ¨¡å‹
    generator = ContextEncoderGenerator(opt)
    discriminator = ContextEncoderDiscriminator(opt)
    
    # åº”ç”¨æƒé‡åˆå§‹åŒ–
    generator.apply(weights_init)
    discriminator.apply(weights_init)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizerD = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizerG = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    corrupted_images = torch.randn(batch_size, 3, 128, 128)
    real_centers = torch.randn(batch_size, 3, 64, 64)
    
    # åˆ›å»ºæ ‡ç­¾
    real_label = torch.full((batch_size, 1), 1.0)
    fake_label = torch.full((batch_size, 1), 0.0)
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.BCELoss()
    
    # é¢„çƒ­
    for _ in range(5):
        # åˆ¤åˆ«å™¨è®­ç»ƒ
        discriminator.zero_grad()
        fake_centers = generator(corrupted_images)
        output_real = discriminator(real_centers)
        errD_real = criterion(output_real, real_label)
        errD_real.backward()
        output_fake = discriminator(fake_centers.detach())
        errD_fake = criterion(output_fake, fake_label)
        errD_fake.backward()
        optimizerD.step()
        
        # ç”Ÿæˆå™¨è®­ç»ƒ
        generator.zero_grad()
        output_fake = discriminator(fake_centers)
        errG_D = criterion(output_fake, real_label)
        errG_D.backward()
        optimizerG.step()
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤æ€§èƒ½
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start_time = time.time()
    
    for _ in range(50):
        # åˆ¤åˆ«å™¨è®­ç»ƒ
        discriminator.zero_grad()
        fake_centers = generator(corrupted_images)
        output_real = discriminator(real_centers)
        errD_real = criterion(output_real, real_label)
        errD_real.backward()
        output_fake = discriminator(fake_centers.detach())
        errD_fake = criterion(output_fake, fake_label)
        errD_fake.backward()
        optimizerD.step()
        
        # ç”Ÿæˆå™¨è®­ç»ƒ
        generator.zero_grad()
        output_fake = discriminator(fake_centers)
        errG_D = criterion(output_fake, real_label)
        errG_D.backward()
        optimizerG.step()
    
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    training_time = time.time() - start_time
    
    print(f"è®­ç»ƒæ­¥éª¤æ€§èƒ½: {training_time/50*1000:.2f} ms/step")
    print(f"ç†è®ºè®­ç»ƒé€Ÿåº¦: {50/training_time:.2f} steps/second")

def benchmark_memory_efficiency():
    """æµ‹è¯•å†…å­˜æ•ˆç‡"""
    print("\næµ‹è¯•å†…å­˜æ•ˆç‡...")
    
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    
    class TestArgs:
        def __init__(self):
            self.ngpu = 1
            self.nc = 3
            self.nef = 64
            self.ngf = 64
            self.ndf = 64
            self.n_bottleneck = 4000
            self.image_size = 128
            self.overlap_pred = 4
    
    opt = TestArgs()
    
    # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜ä½¿ç”¨
    batch_sizes = [1, 2, 4, 8, 16]
    
    for batch_size in batch_sizes:
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # åˆ›å»ºæ¨¡å‹
        generator = ContextEncoderGenerator(opt)
        discriminator = ContextEncoderDiscriminator(opt)
        
        # åˆ›å»ºæ•°æ®
        corrupted_images = torch.randn(batch_size, 3, 128, 128)
        real_centers = torch.randn(batch_size, 3, 64, 64)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            fake_centers = generator(corrupted_images)
            d_real = discriminator(real_centers)
            d_fake = discriminator(fake_centers)
        
        current_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_usage = current_memory - initial_memory
        
        print(f"æ‰¹æ¬¡å¤§å° {batch_size:2d}: {memory_usage:6.2f} MB")
        
        # æ¸…ç†
        del generator, discriminator, corrupted_images, real_centers, fake_centers, d_real, d_fake
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

def benchmark_model_size():
    """æµ‹è¯•æ¨¡å‹å¤§å°"""
    print("\næµ‹è¯•æ¨¡å‹å¤§å°...")
    
    class TestArgs:
        def __init__(self):
            self.ngpu = 1
            self.nc = 3
            self.nef = 64
            self.ngf = 64
            self.ndf = 64
            self.n_bottleneck = 4000
            self.image_size = 128
            self.overlap_pred = 4
    
    opt = TestArgs()
    
    # åˆ›å»ºæ¨¡å‹
    generator = ContextEncoderGenerator(opt)
    discriminator = ContextEncoderDiscriminator(opt)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    gen_params = sum(p.numel() for p in generator.parameters())
    disc_params = sum(p.numel() for p in discriminator.parameters())
    total_params = gen_params + disc_params
    
    # è®¡ç®—æ¨¡å‹å¤§å°ï¼ˆå‡è®¾float32ï¼‰
    gen_size_mb = gen_params * 4 / 1024 / 1024
    disc_size_mb = disc_params * 4 / 1024 / 1024
    total_size_mb = total_params * 4 / 1024 / 1024
    
    print(f"ç”Ÿæˆå™¨å‚æ•°: {gen_params:,} ({gen_size_mb:.2f} MB)")
    print(f"åˆ¤åˆ«å™¨å‚æ•°: {disc_params:,} ({disc_size_mb:.2f} MB)")
    print(f"æ€»å‚æ•°æ•°é‡: {total_params:,} ({total_size_mb:.2f} MB)")
    
    # æµ‹è¯•ä¸åŒé…ç½®çš„æ¨¡å‹å¤§å°
    print("\nä¸åŒé…ç½®çš„æ¨¡å‹å¤§å°å¯¹æ¯”:")
    
    configs = [
        (32, 32, 32, 2000, "å°æ¨¡å‹"),
        (64, 64, 64, 4000, "æ ‡å‡†æ¨¡å‹"),
        (128, 128, 128, 8000, "å¤§æ¨¡å‹")
    ]
    
    for nef, ngf, ndf, n_bottleneck, name in configs:
        class ConfigArgs:
            def __init__(self):
                self.ngpu = 1
                self.nc = 3
                self.nef = nef
                self.ngf = ngf
                self.ndf = ndf
                self.n_bottleneck = n_bottleneck
                self.image_size = 128
                self.overlap_pred = 4
        
        config_opt = ConfigArgs()
        
        gen = ContextEncoderGenerator(config_opt)
        disc = ContextEncoderDiscriminator(config_opt)
        
        gen_params = sum(p.numel() for p in gen.parameters())
        disc_params = sum(p.numel() for p in disc.parameters())
        total_params = gen_params + disc_params
        total_size_mb = total_params * 4 / 1024 / 1024
        
        print(f"{name:8s}: {total_params:8,} å‚æ•° ({total_size_mb:6.2f} MB)")

if __name__ == "__main__":
    print("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    import torch.nn as nn
    
    tests = [
        benchmark_forward_pass,
        benchmark_training_step,
        benchmark_memory_efficiency,
        benchmark_model_size
    ]
    
    for test in tests:
        try:
            test()
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    print("\nğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")
