#!/usr/bin/env python3
"""
æµ‹è¯•è®­ç»ƒæ­¥éª¤å’ŒæŸå¤±å‡½æ•°
"""

import torch
import torch.nn as nn
from model.net import ContextEncoderGenerator, ContextEncoderDiscriminator, weights_init

def test_loss_functions():
    """æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—"""
    print("æµ‹è¯•æŸå¤±å‡½æ•°è®¡ç®—...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    real_centers = torch.randn(batch_size, 3, 64, 64)
    fake_centers = torch.randn(batch_size, 3, 64, 64)
    
    # åˆ›å»ºæ ‡ç­¾
    real_label = torch.full((batch_size, 1), 1.0)
    fake_label = torch.full((batch_size, 1), 0.0)
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.BCELoss()
    criterionMSE = nn.MSELoss()
    
    try:
        # æµ‹è¯•BCEæŸå¤±
        bce_loss = criterion(torch.sigmoid(torch.randn(batch_size, 1)), real_label)
        print(f"âœ… BCEæŸå¤±è®¡ç®—: {bce_loss.item():.4f}")
        
        # æµ‹è¯•MSEæŸå¤±
        mse_loss = criterionMSE(fake_centers, real_centers)
        print(f"âœ… MSEæŸå¤±è®¡ç®—: {mse_loss.item():.4f}")
        
        # æµ‹è¯•L2æŸå¤±ï¼ˆå¸¦æƒé‡çŸ©é˜µï¼‰
        wtl2 = 0.998
        overlapL2Weight = 10
        overlap_pred = 4
        
        wtl2Matrix = real_centers.clone()
        wtl2Matrix.data.fill_(wtl2 * overlapL2Weight)
        center_size = 64
        wtl2Matrix.data[:, :, 
                       overlap_pred:center_size - overlap_pred,
                       overlap_pred:center_size - overlap_pred] = wtl2
        
        l2_loss = (fake_centers - real_centers).pow(2)
        l2_loss = l2_loss * wtl2Matrix
        l2_loss = l2_loss.mean()
        
        print(f"âœ… åŠ æƒL2æŸå¤±è®¡ç®—: {l2_loss.item():.4f}")
        print(f"   æƒé‡çŸ©é˜µå½¢çŠ¶: {wtl2Matrix.shape}")
        print(f"   æƒé‡çŸ©é˜µæ•°å€¼èŒƒå›´: [{wtl2Matrix.min():.3f}, {wtl2Matrix.max():.3f}]")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŸå¤±å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_training_step():
    """æµ‹è¯•å®Œæ•´çš„è®­ç»ƒæ­¥éª¤"""
    print("\næµ‹è¯•å®Œæ•´è®­ç»ƒæ­¥éª¤...")
    
    class TestArgs:
        def __init__(self):
            self.ngpu = 1
            self.nc = 3
            self.nef = 64
            self.ngf = 64
            self.ndf = 64
            self.n_bottleneck = 4000
            self.image_size = 128
            self.overlap_pred = 4
            self.wtl2 = 0.998
            self.overlapL2Weight = 10
    
    opt = TestArgs()
    
    # åˆ›å»ºæ¨¡å‹
    generator = ContextEncoderGenerator(opt)
    discriminator = ContextEncoderDiscriminator(opt)
    
    # åº”ç”¨æƒé‡åˆå§‹åŒ–
    generator.apply(weights_init)
    discriminator.apply(weights_init)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizerD = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizerG = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    corrupted_images = torch.randn(batch_size, 3, 128, 128)
    real_centers = torch.randn(batch_size, 3, 64, 64)
    
    # åˆ›å»ºæ ‡ç­¾
    real_label = torch.full((batch_size, 1), 1.0)
    fake_label = torch.full((batch_size, 1), 0.0)
    
    # å®šä¹‰æŸå¤±å‡½æ•°
    criterion = nn.BCELoss()
    
    try:
        # åˆ¤åˆ«å™¨è®­ç»ƒæ­¥éª¤
        print("  æµ‹è¯•åˆ¤åˆ«å™¨è®­ç»ƒæ­¥éª¤...")
        discriminator.zero_grad()
        
        # çœŸå®å›¾åƒ
        output_real = discriminator(real_centers)
        errD_real = criterion(output_real, real_label)
        errD_real.backward()
        D_x = output_real.mean().item()
        
        # ç”Ÿæˆå›¾åƒ
        fake_centers = generator(corrupted_images)
        output_fake = discriminator(fake_centers.detach())
        errD_fake = criterion(output_fake, fake_label)
        errD_fake.backward()
        D_G_z1 = output_fake.mean().item()
        
        errD = errD_real + errD_fake
        optimizerD.step()
        
        print(f"  âœ… åˆ¤åˆ«å™¨æŸå¤±: {errD.item():.4f}")
        print(f"     D(x): {D_x:.4f}, D(G(z)): {D_G_z1:.4f}")
        
        # ç”Ÿæˆå™¨è®­ç»ƒæ­¥éª¤
        print("  æµ‹è¯•ç”Ÿæˆå™¨è®­ç»ƒæ­¥éª¤...")
        generator.zero_grad()
        
        output_fake = discriminator(fake_centers)
        errG_D = criterion(output_fake, real_label)
        
        # L2æŸå¤±
        wtl2Matrix = real_centers.clone()
        wtl2Matrix.data.fill_(opt.wtl2 * opt.overlapL2Weight)
        wtl2Matrix.data[:, :, opt.overlap_pred:opt.image_size//2 - opt.overlap_pred,
                       opt.overlap_pred:opt.image_size//2 - opt.overlap_pred] = opt.wtl2
        
        errG_l2 = (fake_centers - real_centers).pow(2)
        errG_l2 = errG_l2 * wtl2Matrix
        errG_l2 = errG_l2.mean()
        
        errG = (1 - opt.wtl2) * errG_D + opt.wtl2 * errG_l2
        errG.backward()
        
        D_G_z2 = output_fake.mean().item()
        optimizerG.step()
        
        print(f"  âœ… ç”Ÿæˆå™¨å¯¹æŠ—æŸå¤±: {errG_D.item():.4f}")
        print(f"  âœ… ç”Ÿæˆå™¨L2æŸå¤±: {errG_l2.item():.4f}")
        print(f"  âœ… ç”Ÿæˆå™¨æ€»æŸå¤±: {errG.item():.4f}")
        print(f"     D(G(z)): {D_G_z2:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµ"""
    print("\næµ‹è¯•æ¢¯åº¦æµ...")
    
    class TestArgs:
        def __init__(self):
            self.ngpu = 1
            self.nc = 3
            self.nef = 64
            self.ngf = 64
            self.ndf = 64
            self.n_bottleneck = 4000
            self.image_size = 128
            self.overlap_pred = 4
    
    opt = TestArgs()
    
    # åˆ›å»ºæ¨¡å‹
    generator = ContextEncoderGenerator(opt)
    discriminator = ContextEncoderDiscriminator(opt)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    corrupted_images = torch.randn(2, 3, 128, 128, requires_grad=True)
    real_centers = torch.randn(2, 3, 64, 64)
    
    try:
        # æµ‹è¯•ç”Ÿæˆå™¨æ¢¯åº¦
        fake_centers = generator(corrupted_images)
        loss = fake_centers.mean()
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_grad = corrupted_images.grad is not None
        grad_norm = corrupted_images.grad.norm().item() if has_grad else 0
        
        print(f"âœ… ç”Ÿæˆå™¨æ¢¯åº¦æµ‹è¯•é€šè¿‡")
        print(f"   è¾“å…¥æ¢¯åº¦å­˜åœ¨: {has_grad}")
        print(f"   æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
        
        # æµ‹è¯•åˆ¤åˆ«å™¨æ¢¯åº¦
        discriminator.zero_grad()
        output = discriminator(real_centers)
        loss = output.mean()
        loss.backward()
        
        # æ£€æŸ¥åˆ¤åˆ«å™¨å‚æ•°æ¢¯åº¦
        total_grad_norm = 0
        param_count = 0
        for param in discriminator.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.norm().item()
                param_count += 1
        
        print(f"âœ… åˆ¤åˆ«å™¨æ¢¯åº¦æµ‹è¯•é€šè¿‡")
        print(f"   æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡: {param_count}")
        print(f"   æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\næµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
    
    import psutil
    import os
    
    try:
        # è·å–å½“å‰è¿›ç¨‹
        process = psutil.Process(os.getpid())
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"åˆå§‹å†…å­˜ä½¿ç”¨: {initial_memory:.2f} MB")
        
        # åˆ›å»ºæ¨¡å‹
        class TestArgs:
            def __init__(self):
                self.ngpu = 1
                self.nc = 3
                self.nef = 64
                self.ngf = 64
                self.ndf = 64
                self.n_bottleneck = 4000
                self.image_size = 128
                self.overlap_pred = 4
        
        opt = TestArgs()
        generator = ContextEncoderGenerator(opt)
        discriminator = ContextEncoderDiscriminator(opt)
        
        # è®°å½•æ¨¡å‹åˆ›å»ºåå†…å­˜
        model_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"æ¨¡å‹åˆ›å»ºåå†…å­˜: {model_memory:.2f} MB")
        print(f"æ¨¡å‹å†…å­˜å ç”¨: {model_memory - initial_memory:.2f} MB")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­å†…å­˜
        corrupted_images = torch.randn(4, 3, 128, 128)
        real_centers = torch.randn(4, 3, 64, 64)
        
        with torch.no_grad():
            fake_centers = generator(corrupted_images)
            d_real = discriminator(real_centers)
            d_fake = discriminator(fake_centers)
        
        forward_memory = process.memory_info().rss / 1024 / 1024  # MB
        print(f"å‰å‘ä¼ æ’­åå†…å­˜: {forward_memory:.2f} MB")
        print(f"å‰å‘ä¼ æ’­å†…å­˜å¢åŠ : {forward_memory - model_memory:.2f} MB")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        gen_params = sum(p.numel() for p in generator.parameters())
        disc_params = sum(p.numel() for p in discriminator.parameters())
        total_params = gen_params + disc_params
        
        print(f"âœ… å†…å­˜ä½¿ç”¨æµ‹è¯•é€šè¿‡")
        print(f"   ç”Ÿæˆå™¨å‚æ•°: {gen_params:,}")
        print(f"   åˆ¤åˆ«å™¨å‚æ•°: {disc_params:,}")
        print(f"   æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"   å‚æ•°å†…å­˜ä¼°ç®—: {total_params * 4 / 1024 / 1024:.2f} MB (float32)")
        
        return True
        
    except Exception as e:
        print(f"âŒ å†…å­˜ä½¿ç”¨æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•è®­ç»ƒç›¸å…³åŠŸèƒ½...")
    
    tests = [
        test_loss_functions,
        test_training_step,
        test_gradient_flow,
        test_memory_usage
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print(f"\næµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰è®­ç»ƒæµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
